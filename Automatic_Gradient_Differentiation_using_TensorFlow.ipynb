{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "a7au5xthxPNu",
        "outputId": "e77a49dc-0763-4fad-d2e1-6c3b3380f853"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nAutomatic Differentiation (AD) is a technique used to compute derivatives (gradients) \\nof functions numerically, automatically, and accurately. AD breaks down complex \\nfunctions into a series of elementary operations, and using the chain rule, \\ncalculates the gradients of these operations with respect to the input variables.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Automatic Differentiation/Gradient for Deep Learning\n",
        "\n",
        "\"\"\"\n",
        "Automatic Differentiation (AD) is a technique used to compute derivatives (gradients)\n",
        "of functions numerically, automatically, and accurately. AD breaks down complex\n",
        "functions into a series of elementary operations, and using the chain rule,\n",
        "calculates the gradients of these operations with respect to the input variables.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.version.VERSION"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Cepa18CUzbot",
        "outputId": "e4e397d6-c8ff-42e8-b617-dd2b1c16f9f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Tape : tf.GradientTape\n",
        "\"\"\"\n",
        "This API allows you to record operations executed within a \"tape\" context and\n",
        "then compute gradients of these recorded operations with respect to inputs.\n",
        "\"\"\"\n",
        "x = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x**2\n",
        "\n",
        "\n",
        "# Once you've recorded some operations, use GradientTape.gradient(target, sources)\n",
        "# to calculate the gradient of some target (often a loss) relative to some source.\n",
        "diff_yx = tape.gradient(y, x)   # dy/dx = 2*x\n",
        "diff_yx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0369dH9y9FK",
        "outputId": "f3168918-f9f5-4eb8-c3f1-ded673abba3f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=6.0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.GradientTape works on all tensors, for e.g: y = matmul(x,w) + b\n",
        "w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
        "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
        "x = [[1., 2., 3.]]\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  y = x @ w + b\n",
        "  loss = tf.reduce_mean(y**2)  # mean square loss\n",
        "\n",
        "[dl_dw, dl_db] = tape.gradient(loss, [w, b])\n",
        "print(w)\n",
        "dl_dw, dl_db\n",
        "# Note: The gradient with respect to each source has the shape of the source"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mjge2IFbzHjd",
        "outputId": "ca4d3a80-38f1-46dd-82bc-8b1d84304413"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'w:0' shape=(3, 2) dtype=float32, numpy=\n",
            "array([[0.33232355, 0.52813953],\n",
            "       [0.29861686, 0.61077875],\n",
            "       [0.07380591, 0.11919672]], dtype=float32)>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
              " array([[1.150975 , 2.1072872],\n",
              "        [2.30195  , 4.2145743],\n",
              "        [3.452925 , 6.3218613]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.150975 , 2.1072872], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# variables can be put in dictionary too.\n",
        "my_vars = {\n",
        "    'w': w,\n",
        "    'b': b\n",
        "}\n",
        "\n",
        "grad = tape.gradient(loss, my_vars)\n",
        "grad['b']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY7IjaQ71oo8",
        "outputId": "c6e20969-9513-4fe9-f382-ac1d4d14e19f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.150975 , 2.1072872], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradients with respect to models in ML\n",
        "\"\"\"\n",
        "In machine learning, trainable variables (also known as model parameters or weights)\n",
        "are the parameters that a model learns during training in order to optimize its\n",
        "performance. These variables are adjusted iteratively using optimization algorithms\n",
        "like gradient descent to minimize a loss function.\n",
        "\"\"\"\n",
        "# All subclasses(layers.Layer, keras.Model) of tf.Module aggregate their variables\n",
        "# in the Module.trainable_variables property\n",
        "\n",
        "layer = tf.keras.layers.Dense(2, activation='relu') # fully connected layer\n",
        "x = tf.constant([[1., 2., 3.]])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  # Forward pass\n",
        "  y = layer(x)\n",
        "  loss = tf.reduce_mean(y**2) # mean squared error loss\n",
        "\n",
        "# Gradients with respect to every trainable variable\n",
        "grad = tape.gradient(loss, layer.trainable_variables)\n",
        "\n",
        "for var, g in zip(layer.trainable_variables, grad):\n",
        "  print(f'{var.name}, shape: {g.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65Md0rwa1h3q",
        "outputId": "2df2b9c1-ffb0-4315-f673-e6678b65eb2f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dense/kernel:0, shape: (3, 2)\n",
            "dense/bias:0, shape: (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Controlling What the Tape Watches and Gradient Calculation in TensorFlow\n",
        "\"\"\"\n",
        "Controlling what the tape watches allows you to selectively calculate gradients\n",
        "with respect to specific variables.\n",
        "\"\"\"\n",
        "# Note: By default, the tape watches trainable variables to calculate gradients effectively.\n",
        "\n",
        "\n",
        "\n",
        "x0 = tf.Variable(3.0, name='x0')\n",
        "\n",
        "x1 = tf.Variable(3.0, name='x1', trainable=False) # Not trainable\n",
        "\n",
        "# Not a Variable: A variable + tensor returns a tensor.\n",
        "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
        "\n",
        "x3 = tf.constant(3.0, name='x3') # Not a variable\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = (x0**2) + (x1**2) + (x2**2)\n",
        "\n",
        "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
        "\n",
        "for g in grad:\n",
        "  print(g)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBb8DVBn6HJo",
        "outputId": "52515bf6-6d12-4f48-9c1c-89f1aac81754"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(6.0, shape=(), dtype=float32)\n",
            "None\n",
            "None\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting variable name which are being watched : tape.watched_variables()\n",
        "[var.name for var in tape.watched_variables()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0fB0ErX667Z",
        "outputId": "9e485377-0415-489e-aafa-f1acbb29027a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['x0:0']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In some scenarios, you might want to control which variables are watched by\n",
        "the tf.GradientTape context to control gradient computation.\n",
        "\n",
        "-> Using watch and watch_accessed_variables:\n",
        "\n",
        "To control what the tape watches, you can use the tape.watch(x) function.\n",
        "By calling tape.watch(x) on a specific variable x, you explicitly instruct\n",
        "the tape to watch that variable, even if it's a tensor.\n",
        "\"\"\"\n",
        "\n",
        "with tf.GradientTape() as tape2:\n",
        "  tape2.watch(x1)   # x1 was non- trainable. we can also put x2 and x3\n",
        "  tape2.watch(x3)\n",
        "  y = (x0**2) + (x1**2) + (x2**2)\n",
        "\n",
        "grad2 = tape2.gradient(y, [x0, x1, x2, x3])\n",
        "for g in grad2:\n",
        "  print(g)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lFB8hc77GRY",
        "outputId": "a11c0502-0ab9-43fb-8793-1fe4c038d7ef"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(6.0, shape=(), dtype=float32)\n",
            "tf.Tensor(6.0, shape=(), dtype=float32)\n",
            "None\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# On contrary, To disable the default behavior of watching all tf.Variables, set\n",
        "# watch_accessed_variables=False when creating the gradient tape.\n",
        "\n",
        "x0 = tf.Variable(0.0)\n",
        "x1 = tf.Variable(10.0)\n",
        "\n",
        "with tf.GradientTape(watch_accessed_variables=False) as tape3: # do not watch anyone\n",
        "  tape3.watch(x1)  # watch only x1\n",
        "  y0 = tf.math.sin(x0)\n",
        "  y1 = tf.nn.softplus(x1)\n",
        "  y = y0 + y1\n",
        "  ys = tf.reduce_sum(y)\n",
        "\n",
        "grad3 = tape3.gradient(ys, {'x0': x0, 'x1': x1})\n",
        "print(grad3['x0'])   # no gardient will be computed using x0\n",
        "print(grad3['x1'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw5dEIle8n6j",
        "outputId": "1736733e-c618-4735-96d9-5177dcc1ff4c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "tf.Tensor(0.9999546, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The usage of persistent gradient tapes and Gradients with Respect to Intermediate\n",
        "# values in TensorFlow\n",
        "\"\"\"\n",
        "By default, when you compute gradients using tf.GradientTape, the resources held\n",
        "by the tape are released as soon as the GradientTape.gradient method is called.\n",
        "However, in some situations, you might want to compute gradients multiple times\n",
        "over the same computation. This is where persistent gradient tapes come in.\n",
        "\"\"\"\n",
        "\n",
        "x = tf.constant([1, 3.0])\n",
        "with tf.GradientTape(persistent=True) as tape4: # setting persistent\n",
        "  tape4.watch(x)\n",
        "  y = x * x   # y variable is intermediate variable  = x^2\n",
        "  z = y * y   # final variable = x^4\n",
        "\n",
        "print(tape4.gradient(z, x).numpy())\n",
        "print(tape4.gradient(y, x).numpy())  # intermediate gradients\n",
        "\n",
        "\"\"\"\n",
        "Note: Persistent gradient tapes keep resources allocated until you explicitly\n",
        "release them by deleting the tape.\n",
        "\"\"\"\n",
        "# After using the persistent gradient tape, it's important to explicitly delete\n",
        "# the tape using del tape to release the associated resources.\n",
        "del tape4   # Drop the reference to the tape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9OU6s1zAMC7",
        "outputId": "1464a43b-4e24-41bc-9ae4-9065daa378f3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  4. 108.]\n",
            "[2. 6.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance Study\n",
        "\"\"\"\n",
        "1. Memory Usage with Gradient Tapes:\n",
        "\n",
        " Gradient tapes use memory to store intermediate results, both inputs and outputs,\n",
        " during the forward pass. These intermediate results are necessary for computing\n",
        " gradients during the backward pass (backpropagation).\n",
        "\n",
        "2. Persistent gradient\n",
        " Using persistent=True can be helpful when you need to calculate multiple gradients\n",
        " using the same tape. However, this comes with a trade-off: it increases peak memory usage.\n",
        "\n",
        " \"\"\""
      ],
      "metadata": {
        "id": "QtrvnRzIFOgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradients of Non-Scalar Targets and Element-wise Calculations in TensorFlow\n",
        "\n",
        "# A gradient represents the rate of change of a scalar function with respect to its inputs.\n",
        "\n",
        "\n",
        "x = tf.Variable(2.0)\n",
        "with tf.GradientTape(persistent=True) as tape5:  # set persistent true\n",
        "  y0 = x**2\n",
        "  y1 = 1 / x\n",
        "\n",
        "print(tape5.gradient(y0, x).numpy())\n",
        "print(tape5.gradient(y1, x).numpy())\n",
        "\n",
        "del tape5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDm08Ma-F4uK",
        "outputId": "3a1f9f39-cbcd-4af7-fbf8-be9fa2a316f1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0\n",
            "-0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For the gradient of multiple targets, result for each source is sum of the gradients\n",
        "# of each target\n",
        "\n",
        "# Note: If target are non-scalar applies same, e.g : y = x*[3., 4.] => dy/dx = 3+4\n",
        "\n",
        "x = tf.Variable(2.0)\n",
        "with tf.GradientTape() as tape6:\n",
        "  y0 = x**2   # target 1\n",
        "  y1 = 1 / x  # target 2\n",
        "\n",
        "print(tape6.gradient({'y0': y0, 'y1': y1}, x).numpy())  # dy1/dx + dy0/dx or (d(y0 + y1)/dx)\n",
        "\n",
        "\"\"\"\n",
        "Note: If you need a separate gradient for each item, refer to Jacobians.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LF62QV4oHE51",
        "outputId": "df1d2def-c679-414c-b57a-d6af5ab4fdda"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Control Flow inside GradientTape\n",
        "\"\"\"\n",
        "In scenarios like calculating sub-gradients for functions which are not\n",
        "differentiable at all points, we can calculate gradient using control flow\n",
        "like if/while with different sub-gradients.\n",
        "\"\"\"\n",
        "\n",
        "x = tf.constant(1.5)\n",
        "\n",
        "v0 = tf.Variable(2.0)\n",
        "v1 = tf.Variable(2.0)\n",
        "\n",
        "with tf.GradientTape() as tape7:\n",
        "  tape7.watch(x)\n",
        "  if x > 1.0:      # condition\n",
        "    result = 2*v0   # sub-gradient function\n",
        "  else:\n",
        "    result = v1**2  # another sub-gradient function\n",
        "\n",
        "dv0, dv1 = tape7.gradient(result, [v0, v1])\n",
        "\n",
        "print(dv0)\n",
        "print(dv1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRHaNUvyIcOz",
        "outputId": "11ee8bef-5e9e-4250-8dd6-34e0beb11a1b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(2.0, shape=(), dtype=float32)\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cases where gradient returns None\n",
        "\n",
        "# 1. Replace a Variable with Tensor\n",
        "# Since, GradientTape will automatically watch a tf.Variable but not a tf.Tensor.\n",
        "\n",
        "# 2. Did calculations outside of TensorFlow\n",
        "# The tape can't record the gradient path if the calculation exits TensorFlow.\n",
        "\n",
        "x = tf.Variable([[1.0, 2.0],\n",
        "                 [3.0, 4.0]], dtype=tf.float32)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  x2 = x**2\n",
        "\n",
        "  # This step is calculated with NumPy, not in tensorflow graph\n",
        "  y1 = np.mean(x2, axis=0)\n",
        "\n",
        "  # Like most ops, reduce_mean will cast the NumPy array to a constant tensor\n",
        "  # using `tf.convert_to_tensor`.\n",
        "  y = tf.reduce_mean(y1, axis=0)\n",
        "\n",
        "print(tape.gradient(y, x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQQip2crJknV",
        "outputId": "64e807e5-166c-484a-f0a4-502a61ccd459"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Took gradients through an integer or string\n",
        "# strings and integers are not differentiable\n",
        "x = tf.constant(10)  # integer constant\n",
        "\n",
        "with tf.GradientTape() as g:\n",
        "  y = x * x\n",
        "\n",
        "print(g.gradient(y, x))\n",
        "\n",
        "# Note: We should make this variable watched by tape for getting gradient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1Vdc0WWKsan",
        "outputId": "6311f990-bdb3-4ca2-d716-0572af4a8aa2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Took gradients through a stateful object\n",
        "\"\"\"\n",
        "State stops gradients. When you read from a stateful object, the tape can only\n",
        "observe the current state, not the history that lead to it.\n",
        "\n",
        "-> A tf.Tensor is immutable. You can't change a tensor once it's created.\n",
        "   It has a value, but no state. All the operations discussed so far are also\n",
        "   stateless: the output of a tf.matmul only depends on its inputs.\n",
        "\n",
        "-> A tf.Variable has internal state—its value. When you use the variable, the\n",
        "   state is read. It's normal to calculate a gradient with respect to a variable,\n",
        "   but the variable's state blocks gradient calculations from going farther back.\n",
        "\"\"\"\n",
        "\n",
        "x0 = tf.Variable(3.0)\n",
        "x1 = tf.Variable(0.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "\n",
        "  x1.assign_add(x0) # Update x1 = x1 + x0.\n",
        "\n",
        "  # The tape starts recording from x1.\n",
        "  y = x1**2   # y = (x1 + x0)**2\n",
        "\n",
        "# This doesn't work.\n",
        "print(tape.gradient(y, x0))   #dy/dx0 = 2*(x1 + x0)\n",
        "\n",
        "# Similarly, tf.data.Dataset iterators and tf.queues are stateful, and will stop\n",
        "# all gradients on tensors that pass through them."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byJlMbtjLoVv",
        "outputId": "a8e2c582-f91b-4bba-d9cd-0e0f39a75966"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To avoid non-differentiable conditions of above, We can show 0 instead of None.\n",
        "# we can set flag in gradient = unconnected_gradients=tf.UnconnectedGradients.ZERO)\n",
        "\n",
        "x = tf.Variable([2., 2.])\n",
        "y = tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  z = y**2\n",
        "print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NduRpGnMiFm",
        "outputId": "601c8fad-acd4-4e3e-c0cc-5a869be85a22"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0. 0.], shape=(2,), dtype=float32)\n"
          ]
        }
      ]
    }
  ]
}