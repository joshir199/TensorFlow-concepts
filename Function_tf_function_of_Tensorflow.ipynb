{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qVN06FdFMc35"
      },
      "outputs": [],
      "source": [
        "# Tensorflow Function and Graphs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "tf.version.VERSION"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "I1FYDxLbMmci",
        "outputId": "b5e71ad5-d62a-4776-83f2-d432abdc4454"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorflow graphs\n",
        "\"\"\"\n",
        "In TensorFlow, two modes of execution exist: eager execution and graph execution.\n",
        "\n",
        "1. Eager Execution: This mode executes TensorFlow operations immediately,\n",
        "    operation by operation, within the Python environment. It's more intuitive\n",
        "    for debugging and interactive development but can have some overhead and\n",
        "    might not be as optimized for performance.\n",
        "\n",
        "2. Graph Execution: In this mode, tensor computations are represented as a computational\n",
        "    graph, where operations are nodes and tensors are edges. The entire graph is\n",
        "    optimized and executed, which can lead to better performance, especially for complex\n",
        "    models. Graph execution also allows for portability beyond Python, which is\n",
        "    important for deployment on various platforms.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "A TensorFlow graph is a data structure composed of two main components:\n",
        "\n",
        "1. tf.Operation objects: These represent individual units of computation, like\n",
        "   mathematical operations or layers in a neural network.\n",
        "2. tf.Tensor objects: These represent data that flow between operations, essentially\n",
        "   the inputs and outputs of operations.\n",
        "\n",
        "These components are defined within a tf.Graph context, and together they form a\n",
        "directed acyclic graph (DAG) representing the computation flow.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "68ENbCvLM2a3",
        "outputId": "74153bf1-93da-4184-947e-18c482ca9d45"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nA TensorFlow graph is a data structure composed of two main components:\\n\\n1. tf.Operation objects: These represent individual units of computation, like \\n   mathematical operations or layers in a neural network.\\n2. tf.Tensor objects: These represent data that flow between operations, essentially \\n   the inputs and outputs of operations.\\n\\nThese components are defined within a tf.Graph context, and together they form a \\ndirected acyclic graph (DAG) representing the computation flow.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Benefits of using Graphs\n",
        "\"\"\"\n",
        "1. Portability and Flexibility:\n",
        "\n",
        "With a TensorFlow graph, you gain significant flexibility. You can utilize your\n",
        "computation graph in environments that lack a Python interpreter. This includes\n",
        "scenarios like mobile applications, embedded devices, and backend servers.\n",
        "TensorFlow uses graphs as the fundamental format for saving and exporting models\n",
        "from Python. This portability ensures that models can be deployed and used in a\n",
        "wide range of settings beyond just Python-based environments.\n",
        "\n",
        "2. Graph Optimization:\n",
        "\n",
        "One of the key advantages of graphs is their inherent potential for optimization.\n",
        "TensorFlow's graph execution engine can perform a variety of optimizations,\n",
        "leading to enhanced computational efficiency:\n",
        "\n",
        "  1.Constant Folding: TensorFlow can statically infer the values of tensors by\n",
        "    collapsing constant nodes in your computation. This process is known as constant\n",
        "    folding, which reduces redundant calculations involving constants.\n",
        "  2.Parallel Execution: The graph structure enables TensorFlow to identify sub-parts\n",
        "    of a computation that are independent and execute them concurrently on multiple\n",
        "    threads or devices. This parallelism accelerates the execution of the graph.\n",
        "  3.Common Subexpression Elimination: TensorFlow's optimization system, Grappler,\n",
        "    can identify and eliminate common subexpressions in the computation graph,\n",
        "    simplifying arithmetic operations and further improving efficiency.\n",
        "\n",
        "3. Grappler Optimization System:\n",
        "\n",
        "TensorFlow features an entire optimization system known as Grappler. This system\n",
        "is responsible for performing a wide range of optimizations on the computation graph,\n",
        "including the ones mentioned above. Grappler aims to enhance the speed and efficiency\n",
        "of TensorFlow execution across various hardware platforms.\n",
        "\n",
        "4. Improved Performance:\n",
        "\n",
        "In summary, graphs offer several advantages that contribute to improved performance\n",
        "of TensorFlow models:\n",
        "\n",
        "    Faster Execution: Optimization and parallelism provided by graphs lead to faster\n",
        "    execution times for complex computations.\n",
        "    Parallelism: The ability to identify independent sub-parts of a computation and\n",
        "    execute them concurrently on different threads or devices enhances overall performance.\n",
        "    Efficiency on Multiple Devices: Graphs allow TensorFlow to efficiently utilize\n",
        "    multiple devices, such as GPUs and TPUs, for parallel execution.\n",
        "\n",
        "5. Defining Models in Python:\n",
        "\n",
        "While graphs offer numerous benefits, it's still convenient to define machine learning\n",
        "models (or any computations) in Python. TensorFlow allows you to define models in Python\n",
        "and then automatically constructs the corresponding computation graph when it's needed.\n",
        "This provides the best of both worlds: the convenience of defining models in a high-level\n",
        "language and the efficiency and optimization of graph execution.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "ywOLWLRsNm9U",
        "outputId": "32cd2b76-682d-4a28-9c31-14f41ae56376"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n1. Portability and Flexibility:\\n\\nWith a TensorFlow graph, you gain significant flexibility. You can utilize your \\ncomputation graph in environments that lack a Python interpreter. This includes \\nscenarios like mobile applications, embedded devices, and backend servers. \\nTensorFlow uses graphs as the fundamental format for saving and exporting models \\nfrom Python. This portability ensures that models can be deployed and used in a \\nwide range of settings beyond just Python-based environments.\\n\\n2. Graph Optimization:\\n\\nOne of the key advantages of graphs is their inherent potential for optimization. \\nTensorFlow's graph execution engine can perform a variety of optimizations, \\nleading to enhanced computational efficiency:\\n\\n  1.Constant Folding: TensorFlow can statically infer the values of tensors by \\n    collapsing constant nodes in your computation. This process is known as constant \\n    folding, which reduces redundant calculations involving constants.\\n  2.Parallel Execution: The graph structure enables TensorFlow to identify sub-parts \\n    of a computation that are independent and execute them concurrently on multiple \\n    threads or devices. This parallelism accelerates the execution of the graph.\\n  3.Common Subexpression Elimination: TensorFlow's optimization system, Grappler, \\n    can identify and eliminate common subexpressions in the computation graph, \\n    simplifying arithmetic operations and further improving efficiency.\\n\\n3. Grappler Optimization System:\\n\\nTensorFlow features an entire optimization system known as Grappler. This system \\nis responsible for performing a wide range of optimizations on the computation graph, \\nincluding the ones mentioned above. Grappler aims to enhance the speed and efficiency \\nof TensorFlow execution across various hardware platforms.\\n\\n4. Improved Performance:\\n\\nIn summary, graphs offer several advantages that contribute to improved performance \\nof TensorFlow models:\\n\\n    Faster Execution: Optimization and parallelism provided by graphs lead to faster \\n    execution times for complex computations.\\n    Parallelism: The ability to identify independent sub-parts of a computation and \\n    execute them concurrently on different threads or devices enhances overall performance.\\n    Efficiency on Multiple Devices: Graphs allow TensorFlow to efficiently utilize \\n    multiple devices, such as GPUs and TPUs, for parallel execution.\\n\\n5. Defining Models in Python:\\n\\nWhile graphs offer numerous benefits, it's still convenient to define machine learning \\nmodels (or any computations) in Python. TensorFlow allows you to define models in Python \\nand then automatically constructs the corresponding computation graph when it's needed. \\nThis provides the best of both worlds: the convenience of defining models in a high-level \\nlanguage and the efficiency and optimization of graph execution.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Tensorflow graphs using tf.Function wrapper\n",
        "# A Function is a Python callable that builds TensorFlow graphs from the Python\n",
        "# function. You use a Function in the same way as its Python equivalent.\n",
        "\n",
        "# Define a Python function.\n",
        "def some_regular_function(x, y, b):\n",
        "  x = tf.matmul(x, y)\n",
        "  x = x + b\n",
        "  return x\n",
        "\n",
        "# `a_function_that_uses_a_graph` is a TensorFlow `Function`.\n",
        "a_function_that_uses_a_graph = tf.function(some_regular_function)\n",
        "\n",
        "# Make some tensors.\n",
        "x1 = tf.constant([[1.0, 2.0]])\n",
        "y1 = tf.constant([[2.0], [3.0]])\n",
        "b1 = tf.constant(4.0)\n",
        "\n",
        "orig_value = some_regular_function(x1, y1, b1).numpy()\n",
        "# Call a `Function` like a Python function.\n",
        "tf_function_value = a_function_that_uses_a_graph(x1, y1, b1).numpy()\n",
        "assert(orig_value == tf_function_value)  # asserts True"
      ],
      "metadata": {
        "id": "9fwqskVYOAlH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.function applies to a function and all other functions it calls.\n",
        "\n",
        "def inner_function(x, y, b):\n",
        "  x = tf.matmul(x, y)\n",
        "  x = x + b\n",
        "  return x\n",
        "\n",
        "# Use the decorator to make `outer_function` a `Function`.\n",
        "@tf.function\n",
        "def outer_function(x):\n",
        "  y = tf.constant([[2.0], [3.0]])\n",
        "  b = tf.constant(4.0)\n",
        "\n",
        "  return inner_function(x, y, b)\n",
        "\n",
        "# Note that the callable will create a graph that\n",
        "# includes `inner_function` as well as `outer_function`.\n",
        "outer_function(tf.constant([[1.0, 2.0]])).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk9O3uqNPU6d",
        "outputId": "e2f35fd9-53db-4958-be3b-682877e5f9f3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[12.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note:  tf.function uses a library called AutoGraph (tf.autograph) to convert\n",
        "# Python code into graph-generating code.\n",
        "\n",
        "print(\"First branch, with graph:\", outer_function(tf.constant([[1.0, 2.0]])).numpy())\n",
        "print(\"Second branch, with graph:\", outer_function(tf.constant([[1.0, -2.0]])).numpy())\n",
        "\n",
        "def simple_relu(x):\n",
        "  if tf.greater(x, 0):\n",
        "    return x\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "# `tf_simple_relu` is a TensorFlow `Function` that wraps `simple_relu`.\n",
        "tf_simple_relu = tf.function(simple_relu)\n",
        "\n",
        "print(\"First branch, with graph:\", tf_simple_relu(tf.constant(1)).numpy())\n",
        "print(\"Second branch, with graph:\", tf_simple_relu(tf.constant(-1)).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivcPVuIlPj0D",
        "outputId": "54aa0e92-2ad0-4a34-9ad9-2b79106d9195"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First branch, with graph: [[12.]]\n",
            "Second branch, with graph: [[0.]]\n",
            "First branch, with graph: 1\n",
            "Second branch, with graph: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# This is the graph-generating output of AutoGraph.\n",
        "print(tf.autograph.to_code(simple_relu)) # only apply on open function before\n",
        "                                            # converting it into graph."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj4l30uXQBVE",
        "outputId": "5a2a8c28-51f6-48af-f788-8c91afb08358"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def tf__simple_relu(x):\n",
            "    with ag__.FunctionScope('simple_relu', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
            "        do_return = False\n",
            "        retval_ = ag__.UndefinedReturnValue()\n",
            "\n",
            "        def get_state():\n",
            "            return (do_return, retval_)\n",
            "\n",
            "        def set_state(vars_):\n",
            "            nonlocal retval_, do_return\n",
            "            (do_return, retval_) = vars_\n",
            "\n",
            "        def if_body():\n",
            "            nonlocal retval_, do_return\n",
            "            try:\n",
            "                do_return = True\n",
            "                retval_ = ag__.ld(x)\n",
            "            except:\n",
            "                do_return = False\n",
            "                raise\n",
            "\n",
            "        def else_body():\n",
            "            nonlocal retval_, do_return\n",
            "            try:\n",
            "                do_return = True\n",
            "                retval_ = 0\n",
            "            except:\n",
            "                do_return = False\n",
            "                raise\n",
            "        ag__.if_stmt(ag__.converted_call(ag__.ld(tf).greater, (ag__.ld(x), 0), None, fscope), if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)\n",
            "        return fscope.ret(retval_, do_return)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets understand how A Function encapsulates several tf.Graphs behind one API\n",
        "# using polymorphism and make it efficient execution and deployable.\n",
        "\"\"\"\n",
        "A tf.Graph is specialized to a specific type of inputs (for example, tensors with\n",
        "a specific dtype or objects with the same id()).\n",
        "\n",
        "Each time you invoke a Function with a set of arguments that can't be handled by\n",
        "any of its existing graphs (such as arguments with new dtypes or incompatible shapes),\n",
        "Function creates a new tf.Graph specialized to those new arguments. The type\n",
        "specification of a tf.Graph's inputs is known as its input signature or just a signature.\n",
        "\"\"\"\n",
        "\n",
        "# The most notable feature of a Function is its ability to encapsulate several tf.Graphs\n",
        "# behind a single API. Each tf.Graph represents a computation graph that defines the\n",
        "# operations and data flow for a specific computation. By encapsulating multiple graphs\n",
        "# within a Function, you gain the ability to create a unified and optimized execution environment.\n",
        "\n",
        "@tf.function\n",
        "def my_relu(x):\n",
        "  return tf.maximum(0., x)\n",
        "\n",
        "# `my_relu` creates new graphs as it observes more signatures.\n",
        "print(my_relu(tf.constant(5.5)))\n",
        "print(my_relu([1, -1]))\n",
        "print(my_relu(tf.constant([3., -3.])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTFSlJSZRR8q",
        "outputId": "1dec9ba5-8b60-4cbb-ae46-6e678f0f84aa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(5.5, shape=(), dtype=float32)\n",
            "tf.Tensor([1. 0.], shape=(2,), dtype=float32)\n",
            "tf.Tensor([3. 0.], shape=(2,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If the Function has already been called with that signature, Function does not\n",
        "# create a new tf.Graph.\n",
        "print(my_relu([9, -2]))   # new graph, has unique input (not a tensor, but python argument)\n",
        "print(my_relu(tf.constant([9.0, 6.5])))  # no new graph, has same tensor type\n",
        "print(my_relu(tf.constant(6.0)))  # no new graph, has same tensor type\n",
        "\n",
        "# Note: New Python arguments always trigger the creation of a new graph, hence the extra tracing.\n",
        "print(my_relu.pretty_printed_concrete_signatures())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flTM8ZY7RszC",
        "outputId": "02b2268e-ce3b-4247-e1b0-4ac2b8ec5358"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([9. 0.], shape=(2,), dtype=float32)\n",
            "tf.Tensor([9.  6.5], shape=(2,), dtype=float32)\n",
            "tf.Tensor(6.0, shape=(), dtype=float32)\n",
            "my_relu(x)\n",
            "  Args:\n",
            "    x: float32 Tensor, shape=()\n",
            "  Returns:\n",
            "    float32 Tensor, shape=()\n",
            "\n",
            "my_relu(x=[1, -1])\n",
            "  Returns:\n",
            "    float32 Tensor, shape=(2,)\n",
            "\n",
            "my_relu(x)\n",
            "  Args:\n",
            "    x: float32 Tensor, shape=(2,)\n",
            "  Returns:\n",
            "    float32 Tensor, shape=(2,)\n",
            "\n",
            "my_relu(x=[9, -2])\n",
            "  Returns:\n",
            "    float32 Tensor, shape=(2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# different scanrios of using tf.function\n",
        "# 1. Graph execution vs. eager execution\n",
        "\n",
        "\"\"\"\n",
        "The code in a Function can be executed both eagerly and as a graph.\n",
        "By default, Function executes its code as a graph\n",
        "\"\"\"\n",
        "# To make it execute eagerly, It can be set with tf.config.run_functions_eagerly(True)\n",
        "# This is a switch that turns off Function's ability to create and run graphs,\n",
        "# instead of executing the code normally.\n",
        "\n",
        "\"\"\"\n",
        "However, Function can behave differently under graph and eager execution.\n",
        "The Python print function is one example of how these two modes differ. Let's check\n",
        "out what happens when you insert a print statement to your function and call it repeatedly.\n",
        "\"\"\"\n",
        "\n",
        "@tf.function\n",
        "def get_MSE(y_true, y_pred):\n",
        "  print(\"Calculating MSE!\")\n",
        "  sq_diff = tf.pow(y_true - y_pred, 2)\n",
        "  return tf.reduce_mean(sq_diff)\n",
        "\n",
        "y_true = tf.random.uniform([5], maxval=10, dtype=tf.int32)\n",
        "y_pred = tf.random.uniform([5], maxval=10, dtype=tf.int32)\n",
        "\n",
        "error1 = get_MSE(y_true, y_pred)\n",
        "error2 = get_MSE(y_true, y_pred)\n",
        "error3 = get_MSE(y_true, y_pred)\n",
        "# Print is getting executed only once"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEKqrwNMTjeU",
        "outputId": "e38343a5-a8c9-448c-b2b8-d79ad40f971b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating MSE!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The print statement is executed when Function runs the original code in order\n",
        "# to create the graph in a process known as \"tracing\".\n",
        "\"\"\"\n",
        "Tracing captures the TensorFlow operations into a graph, and print is not captured in the\n",
        "graph. That graph is then executed for all three calls without ever running the Python code again.\n",
        "\"\"\"\n",
        "# Now, globally set everything to run eagerly to force eager execution.\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "error1 = get_MSE(y_true, y_pred)\n",
        "error2 = get_MSE(y_true, y_pred)\n",
        "error3 = get_MSE(y_true, y_pred)\n",
        "# It will print 3 times as it runs as python code not as graph\n",
        "# Better solution is to use tf.print() which will be included in graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFqVBCvaVG0L",
        "outputId": "16b0f680-1e89-4a81-faf3-8cdd9ca05442"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating MSE!\n",
            "Calculating MSE!\n",
            "Calculating MSE!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-Strict Execution in TensorFlow: Understanding Graph Execution Behavior\n",
        "\n",
        "\"\"\"\n",
        "1. Non-Strict Execution vs. Eager Execution:\n",
        "\n",
        "In TensorFlow, two primary execution modes exist: non-strict execution (graph execution)\n",
        "and eager execution. Eager execution involves executing each operation as it appears in\n",
        "the code, step by step, allowing for immediate feedback and debugging. Non-strict\n",
        "execution, on the other hand, is characteristic of graph execution, where only operations\n",
        "necessary to produce observable effects are executed.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "2. Observable Effects and Non-Strict Execution:\n",
        "\n",
        "In non-strict execution, only operations that lead to observable effects are executed.\n",
        "These observable effects include:\n",
        "\n",
        "  1. The return value of the function.\n",
        "  2. Well-known side-effects, such as:\n",
        "       -> Input/output operations like tf.print.\n",
        "      -> Debugging operations like assertion functions in tf.debugging.\n",
        "      -> Mutations of tf.Variable.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "3. Skipping Unnecessary Operations:\n",
        "\n",
        "Graph execution only performs operations that contribute to producing the observable\n",
        "effects mentioned above. If an operation doesn't affect any of these effects, it's\n",
        "skipped during graph execution, leading to potential optimizations in terms of\n",
        "execution time and computational resources.\n",
        "\n",
        "4. Runtime Error Handling:\n",
        "\n",
        "An important aspect of non-strict execution is that runtime error checking is not\n",
        "counted as an observable effect. If an operation is skipped during graph execution\n",
        "because it's deemed unnecessary, it won't raise any runtime errors related to that operation.\n",
        "\"\"\"\n",
        "tf.config.run_functions_eagerly(False)  # make it false if it is already True\n",
        "\n",
        "@tf.function\n",
        "def unused_return_graph(x):\n",
        "  tf.gather(x, [1]) # unused\n",
        "  return x\n",
        "\n",
        "# Only needed operations are run during graph execution. The error is not raised.\n",
        "print(unused_return_graph(tf.constant([0.0])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOxci5k8Vsjx",
        "outputId": "0bc3f0e8-fd48-4243-a5d1-e42a676eab09"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0.], shape=(1,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# But with Eager execution, It will show error.\n",
        "def unused_return_eager(x):\n",
        "  # Get index 1 will fail when `len(x) == 1`\n",
        "  tf.gather(x, [1]) # unused\n",
        "  return x\n",
        "\n",
        "try:\n",
        "  print(unused_return_eager(tf.constant([0.0])))\n",
        "except tf.errors.InvalidArgumentError as e:\n",
        "  # All operations are run during eager execution so an error is raised.\n",
        "  print(f'{type(e).__name__}: {e}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_VakQVjZMKI",
        "outputId": "52b71abe-016c-412d-eecb-8a1e746788d8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "InvalidArgumentError: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0] = 1 is not in [0, 1) [Op:GatherV2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Speed Enhancement using graph execution:\n",
        "\"\"\"\n",
        "tf.function usually improves the performance of your code, but the amount of speed-up\n",
        "depends on the kind of computation you run. Small computations can be dominated by\n",
        "the overhead of calling a graph.\n",
        "\n",
        "Graphs can speed up your code, but the process of creating them has some overhead.\n",
        "For some functions, the creation of the graph takes more time than the execution of the graph.\n",
        "\n",
        "Note: No matter how large your model, you want to avoid tracing frequently or multiple times.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "sCkESL7sZR_C",
        "outputId": "b042127f-e40f-458e-d34c-f0855851996a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntf.function usually improves the performance of your code, but the amount of speed-up \\ndepends on the kind of computation you run. Small computations can be dominated by \\nthe overhead of calling a graph.\\n\\nGraphs can speed up your code, but the process of creating them has some overhead. \\nFor some functions, the creation of the graph takes more time than the execution of the graph.\\n\\nNote: No matter how large your model, you want to avoid tracing frequently or multiple times.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Understanding TensorFlow Functions tf.function in more detail\n",
        "\"\"\"\n",
        "The main takeaways and recommendations are:\n",
        "\n",
        "1. Debug in eager mode, then decorate with @tf.function.\n",
        "2. Don't rely on Python side effects like object mutation or list appends.\n",
        "3. tf.function works best with TensorFlow ops; NumPy and Python calls are converted to constants.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "rBclPUaOcFJs",
        "outputId": "1e634d26-b065-43ab-82c7-12fb0fc1e2fd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nThe main takeaways and recommendations are:\\n\\n1. Debug in eager mode, then decorate with @tf.function.\\n2. Don't rely on Python side effects like object mutation or list appends.\\n3. tf.function works best with TensorFlow ops; NumPy and Python calls are converted to constants.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tracing in tensorflow Graph execution using tf.function\n",
        "\"\"\"\n",
        "In TensorFlow, computations are usually executed within a computational graph, which\n",
        "allows for optimizations like parallel execution and device placement. However, not all\n",
        "Python code can be directly represented in a TensorFlow graph due to differences in\n",
        "execution models and data types.\n",
        " e.g: Raising an error, or working with a more complex Python object; none of these\n",
        "      things can run in a tf.Graph\n",
        "\"\"\"\n",
        "\n",
        "# Function bridges this gap by separating your code in two stages:\n",
        "\"\"\"\n",
        "Tracing Stage:\n",
        "   In this stage, a new TensorFlow graph (tf.Graph) is created. During tracing,\n",
        "   TensorFlow operations within the function are deferred â€“ they are captured by\n",
        "   the graph and not immediately executed. Python code runs normally, but TensorFlow\n",
        "   operations are recorded for later execution in the graph.\n",
        "\n",
        "Execution Stage:\n",
        "   In this stage, the traced graph is executed. This stage is much faster than the\n",
        "   tracing stage because the graph is optimized and ready for efficient execution.\n",
        "\n",
        "\"\"\"\n",
        "# Depending on its inputs,, Skipping the first stage and only executing the second\n",
        "# stage is what gives you TensorFlow's high performance.\n",
        "\n",
        "# When you pass arguments of different types into a Function, both stages are run:\n",
        "\n",
        "@tf.function\n",
        "def double(a):\n",
        "  print(\"Tracing with\", a)\n",
        "  return a + a\n",
        "\n",
        "print(double(tf.constant(1)))\n",
        "print()\n",
        "print(double(tf.constant(1.1)))\n",
        "print()\n",
        "print(double(tf.constant(\"a\")))\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGn3hsJXdXO3",
        "outputId": "bd501a33-030c-475f-b089-1ee8f9b290c4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing with Tensor(\"a:0\", shape=(), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "\n",
            "Tracing with Tensor(\"a:0\", shape=(), dtype=float32)\n",
            "tf.Tensor(2.2, shape=(), dtype=float32)\n",
            "\n",
            "Tracing with Tensor(\"a:0\", shape=(), dtype=string)\n",
            "tf.Tensor(b'aa', shape=(), dtype=string)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that if you repeatedly call a Function with the same argument type,\n",
        "# TensorFlow will skip the tracing stage and reuse a previously traced graph, as\n",
        "# the generated graph would be identical.\n",
        "print(double(tf.constant(\"b\")))\n",
        "print(double(tf.constant(\"c\")))\n",
        "print(double(tf.constant(\"d\")))  # tracing is not called as their graph already created"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBFAk2woewLs",
        "outputId": "65828e35-6a68-4306-cfaf-7f4520391700"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'bb', shape=(), dtype=string)\n",
            "tf.Tensor(b'cc', shape=(), dtype=string)\n",
            "tf.Tensor(b'dd', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To see all of the available traces\n",
        "print(double.pretty_printed_concrete_signatures())  # 3 traces are made"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJTH2joxfRRH",
        "outputId": "c67068a7-8f31-458a-b957-6287e873e64b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "double(a)\n",
            "  Args:\n",
            "    a: int32 Tensor, shape=()\n",
            "  Returns:\n",
            "    int32 Tensor, shape=()\n",
            "\n",
            "double(a)\n",
            "  Args:\n",
            "    a: float32 Tensor, shape=()\n",
            "  Returns:\n",
            "    float32 Tensor, shape=()\n",
            "\n",
            "double(a)\n",
            "  Args:\n",
            "    a: string Tensor, shape=()\n",
            "  Returns:\n",
            "    string Tensor, shape=()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  tf.function creates a cached, dynamic dispatch layer over TensorFlow's graph tracing logic\n",
        "\"\"\"\n",
        "1. A tf.Graph is the raw, language-agnostic, portable representation of a TensorFlow computation.\n",
        "2. A ConcreteFunction wraps a tf.Graph.\n",
        "3. A Function manages a cache of ConcreteFunctions and picks the right one for your inputs.\n",
        "   tf.function wraps a Python function, returning a Function object.\n",
        "4. Tracing creates a tf.Graph and wraps it in a ConcreteFunction, also known as a trace.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RzO-nQbLffr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rules of Tracing in tf.function\n",
        "# Tracing is a pivotal step in the functioning of Function, as it involves creating\n",
        "# and reusing ConcreteFunctions for optimal performance.\n",
        "\"\"\"\n",
        "\n",
        "1. Matching Call Arguments to ConcreteFunctions:\n",
        "\n",
        "When a Function is called, it attempts to match the call arguments to existing\n",
        "ConcreteFunctions. This matching is done using tf.types.experimental.TraceType\n",
        "associated with each argument. If a matching ConcreteFunction is found, the call\n",
        "is dispatched to it. If no match exists, a new ConcreteFunction is traced.\n",
        "\n",
        "2. Subtyping and Specific Signatures:\n",
        "\n",
        "Tracing selects the most specific signature when multiple matches are found.\n",
        "Subtyping rules are employed for matching, similar to how function calls work in\n",
        "languages like C++ or Java. For instance, if a TensorShape([1, 2]) argument is passed,\n",
        "a ConcreteFunction with a broader shape like TensorShape([None, None]) can be matched,\n",
        "but if a more specific one like TensorShape([1, None]) exists, it will be prioritized.\n",
        "\n",
        "3. Determining TraceType for Different Inputs:\n",
        "\n",
        "The TraceType for various input types is determined as follows:\n",
        "\n",
        "  ->  For Tensor, the type is determined by its dtype and shape. Ranked shapes are\n",
        "    subtypes of unranked shapes, and fixed dimensions are subtypes of unknown dimensions.\n",
        "  ->  For Variable, the type is similar to a Tensor, but includes a unique resource\n",
        "    ID of the variable to handle control dependencies correctly.\n",
        "  ->  For Python primitive values, the type corresponds directly to the value itself.\n",
        "    For example, the TraceType of the value 3 is LiteralTraceType<3>, not int.\n",
        "  ->  For Python ordered containers like lists and tuples, the type depends on the\n",
        "    types of their elements. For instance, the type of [1, 2] is ListTraceType<LiteralTraceType<1>, LiteralTraceType<2>>.\n",
        "  ->  For Python mappings like dictionaries, the type is a mapping from the same\n",
        "    keys to the types of values instead of the actual values themselves.\n",
        "  ->  For Python objects implementing the __tf_tracing_type__ method, the type is\n",
        "    determined by the return value of that method.\n",
        "  ->  For other Python objects, a generic TraceType is used. The matching procedure\n",
        "    involves checking if the object is the same as the one used in the previous\n",
        "    trace or if it's equal to the previous trace's object.\n",
        "\n",
        "4. Importance of Immutability:\n",
        "\n",
        "When using Python objects as arguments to tf.function, it's recommended to use immutable objects.\n",
        "The procedure for checking object identity and equality relies on this. Weak references\n",
        "to the object are maintained, so the object must remain in scope and not be deleted.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Aj75UuuQBARy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Controlling retracing\n",
        "# If your Function retraces a new graph for every call, you'll find that your code\n",
        "# executes more slowly than if you didn't use tf.function.\n",
        "\n",
        "# 1. Pass a fixed input_signature to tf.function\n",
        "\n",
        "@tf.function(input_signature=(tf.TensorSpec(shape=[None], dtype=tf.int32),))\n",
        "def next_collatz(x):\n",
        "  print(\"Tracing with\", x)\n",
        "  return tf.where(x % 2 == 0, x // 2, 3 * x + 1)\n",
        "\n",
        "print(next_collatz(tf.constant([1, 2])))\n",
        "\n",
        "# You specified a 1-D tensor in the input signature, so this should fail (dim > 1).\n",
        "#next_collatz(tf.constant([[1, 2], [3, 4]]))\n",
        "\n",
        "# You specified an int32 dtype in the input signature, so this should fail.(wrong dtype)\n",
        "#next_collatz(tf.constant([1.0, 2.0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx3spL8DB_gu",
        "outputId": "470f47e3-be26-4e31-82ad-12de531b84b8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing with Tensor(\"x:0\", shape=(None,), dtype=int32)\n",
            "tf.Tensor([4 1], shape=(2,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Use unknown dimensions for flexibility\n",
        "# Since TensorFlow matches tensors based on their shape, using a None dimension\n",
        "# as a wildcard will allow Functions to reuse traces for variably-sized input.\n",
        "\n",
        "@tf.function(input_signature=(tf.TensorSpec(shape=[None], dtype=tf.int32),))\n",
        "def g(x):\n",
        "  print('Tracing with', x)\n",
        "  return x\n",
        "\n",
        "# No retrace!\n",
        "print(g(tf.constant([1, 2, 3])))\n",
        "print(g(tf.constant([1, 2, 3, 4, 5])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhMRPL6CC0J7",
        "outputId": "28f3b189-d5d8-44e6-fd83-b7c772200a65"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing with Tensor(\"x:0\", shape=(None,), dtype=int32)\n",
            "tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n",
            "tf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Pass tensors instead of python literals\n",
        "# We should avoid passing python arguments, rather pass tensors.\n",
        "\n",
        "def train_one_step():\n",
        "  pass\n",
        "\n",
        "@tf.function\n",
        "def train(num_steps):\n",
        "  print(\"Tracing with num_steps = \", num_steps)\n",
        "  tf.print(\"Executing with num_steps = \", num_steps)\n",
        "  for _ in tf.range(num_steps):\n",
        "    train_one_step()\n",
        "\n",
        "print(\"Retracing occurs for different Python arguments.\")\n",
        "train(num_steps=10)\n",
        "train(num_steps=20)\n",
        "\n",
        "print()\n",
        "print(\"Traces are reused for Tensor arguments.\")\n",
        "train(num_steps=tf.constant(10))\n",
        "train(num_steps=tf.constant(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csmog4qzDDd2",
        "outputId": "dfdd6d5e-6a54-4a1c-8e46-cc1406430254"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retracing occurs for different Python arguments.\n",
            "Tracing with num_steps =  10\n",
            "Executing with num_steps =  10\n",
            "Tracing with num_steps =  20\n",
            "Executing with num_steps =  20\n",
            "\n",
            "Traces are reused for Tensor arguments.\n",
            "Tracing with num_steps =  Tensor(\"num_steps:0\", shape=(), dtype=int32)\n",
            "Executing with num_steps =  10\n",
            "Executing with num_steps =  20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you need to force retracing, create a new Function. Separate Function objects\n",
        "# are guaranteed not to share traces.\n",
        "def f():\n",
        "  print('Tracing!')\n",
        "  tf.print('Executing')\n",
        "\n",
        "tf.function(f)()  # new Function object\n",
        "tf.function(f)()  # new Function object"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMMPIwmtD_wl",
        "outputId": "a26df3d9-6fc8-4fdd-e3ef-b6db01af1853"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing!\n",
            "Executing\n",
            "Tracing!\n",
            "Executing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtaining concrete functions\n",
        "\"\"\"\n",
        "Every time a function is traced, a new concrete function is created. You can directly\n",
        "obtain a concrete function, by using get_concrete_function.\n",
        "\"\"\"\n",
        "print(\"Obtaining concrete trace\")\n",
        "double_strings = double.get_concrete_function(tf.constant(\"a\"))\n",
        "print(\"Executing traced function\")\n",
        "print(double_strings(tf.constant(\"a\")))\n",
        "print(double_strings(a=tf.constant(\"b\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6qlmJh-EUN-",
        "outputId": "cd1bc7e3-9d4d-4ada-a011-b889fd7e3278"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining concrete trace\n",
            "Executing traced function\n",
            "tf.Tensor(b'aa', shape=(), dtype=string)\n",
            "tf.Tensor(b'bb', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing a ConcreteFunction displays a summary of its input arguments (with types) and its output type.\n",
        "print(double_strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaZaiZZWEzEq",
        "outputId": "f34f1b6f-f4b6-4d5d-aeab-02cc8aecfac4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConcreteFunction double(a)\n",
            "  Args:\n",
            "    a: string Tensor, shape=()\n",
            "  Returns:\n",
            "    string Tensor, shape=()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtaining graphs\n",
        "# Each concrete function is a callable wrapper around a tf.Graph\n",
        "graph = double_strings.graph\n",
        "for node in graph.as_graph_def().node:\n",
        "  print(f'{node.input} -> {node.name}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Zoy9_NKFKB-",
        "outputId": "845bcc98-b2ed-4e75-e23a-9e8e00f55cc2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[] -> a\n",
            "['a', 'a'] -> add\n",
            "['add'] -> Identity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging\n",
        "\"\"\"\n",
        "In general, debugging code is easier in eager mode than inside tf.function. You should\n",
        "ensure that your code executes error-free in eager mode before decorating with tf.function.\n",
        "\"\"\"\n",
        "# To assist in the debugging process, you can call tf.config.run_functions_eagerly(True)\n",
        "# to globally disable and reenable tf.function.\n",
        "\n",
        "\"\"\"\n",
        "When tracking down issues that only appear within tf.function, here are some tips:\n",
        "\n",
        "-> Plain old Python print calls only execute during tracing, helping you track down\n",
        "  when your function gets (re)traced.\n",
        "-> tf.print calls will execute every time, and can help you track down intermediate\n",
        "  values during execution.\n",
        "-> tf.debugging.enable_check_numerics is an easy way to track down where NaNs and Inf are created.\n",
        "-> pdb (the Python debugger) can help you understand what's going on during tracing.\n",
        "  (Caveat: pdb will drop you into AutoGraph-transformed source code.)"
      ],
      "metadata": {
        "id": "30wJFpWOFXVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AutoGraph is a library of TensorFlow that automatically converts certain Python\n",
        "# constructs, like loops and conditionals, into equivalent TensorFlow operations.\n",
        "# When working with conditionals, AutoGraph has a distinct behavior depending on\n",
        "# whether the condition is a TensorFlow Tensor or a Python value.\n",
        "\n",
        "@tf.function\n",
        "def fizzbuzz(n):\n",
        "  for i in tf.range(1, n + 1):\n",
        "    print('Tracing for loop')\n",
        "    if i % 15 == 0:\n",
        "      print('Tracing fizzbuzz branch')\n",
        "      tf.print('fizzbuzz')\n",
        "    elif i % 3 == 0:\n",
        "      print('Tracing fizz branch')\n",
        "      tf.print('fizz')\n",
        "    elif i % 5 == 0:\n",
        "      print('Tracing buzz branch')\n",
        "      tf.print('buzz')\n",
        "    else:\n",
        "      print('Tracing default branch')\n",
        "      tf.print(i)\n",
        "\n",
        "fizzbuzz(tf.constant(5))\n",
        "print()\n",
        "fizzbuzz(tf.constant(2))\n",
        "\n",
        "\"\"\"\n",
        "tf.cond is a TensorFlow operation that traces and adds all branches of a conditional\n",
        "to the graph. That is why print statement will print from each branch at graph creation.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "wW-zSvguF_NE",
        "outputId": "dbaafe47-4706-46b8-c069-603650f49265"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing for loop\n",
            "Tracing fizzbuzz branch\n",
            "Tracing fizz branch\n",
            "Tracing buzz branch\n",
            "Tracing default branch\n",
            "1\n",
            "2\n",
            "fizz\n",
            "4\n",
            "buzz\n",
            "\n",
            "1\n",
            "2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntf.cond is a TensorFlow operation that traces and adds all branches of a conditional \\nto the graph. That is why print statement will print from each branch at graph creation.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarily, we have tf.while_loop for for/while loop of python\n",
        "\"\"\"\n",
        "A Python loop executes during tracing, adding additional ops to the tf.Graph for\n",
        "every iteration of the loop.\n",
        "\n",
        "A TensorFlow loop traces the body of the loop, and dynamically selects how many\n",
        "iterations to run at execution time. The loop body only appears once in the generated tf.Graph.\n",
        "\"\"\"\n",
        "\n",
        "@tf.function\n",
        "def f(x):\n",
        "  while tf.reduce_sum(x) > 1:\n",
        "    tf.print(x)\n",
        "    x = tf.tanh(x)\n",
        "  return x\n",
        "\n",
        "f(tf.random.uniform([2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-x1PdfzyGrG0",
        "outputId": "a54bb5ca-4b0e-423c-ec5b-54a8e873dd98"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.27705026, 0.0154227 ], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.autograph.to_code(fizzbuzz.python_function))"
      ],
      "metadata": {
        "id": "RMZr-hLOG2jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# common Pitfall of using tf.function on Loop functions\n",
        "\"\"\"\n",
        "A common pitfall is to loop over Python/NumPy data within a tf.function. This loop will\n",
        "execute during the tracing process, adding a copy of your model to the tf.Graph for each\n",
        "iteration of the loop.\n",
        "\n",
        "If you want to wrap the entire training loop in tf.function, the safest way to do this is\n",
        "to wrap your data as a tf.data.Dataset so that AutoGraph will dynamically unroll the\n",
        "training loop.\n",
        "\n",
        "\"\"\"\n",
        "# Reading data from files via TFRecordDataset, CsvDataset, etc. is the most effective\n",
        "# way to consume data, as then TensorFlow itself can manage the asynchronous loading\n",
        "# and prefetching of data, without having to involve Python."
      ],
      "metadata": {
        "id": "w7NtT4Q5J_NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Limitations of using tf.function\n",
        "\n",
        "# TensorFlow Function has a few limitations by design that you should be aware of\n",
        "# when converting a Python function to a Function.\n",
        "\n",
        "\"\"\"\n",
        "1. Executing Python side effects\n",
        "Side effects, like printing, appending to lists, and mutating globals, can behave\n",
        "unexpectedly inside a Function, sometimes executing twice or not all. They only happen\n",
        "the first time you call a Function with a set of inputs. Afterwards, the traced\n",
        "tf.Graph is reexecuted, without executing the Python code.\n",
        "\n",
        "-> The general rule of thumb is to avoid relying on Python side effects in your logic\n",
        "   TensorFlow APIs like tf.data, tf.print, tf.summary, tf.Variable.assign, and tf.TensorArray\n",
        "   are the best way to ensure your code will be executed by the TensorFlow runtime with each call.\n",
        "-> If you would like to execute Python code during each invocation of a Function, tf.py_function\n",
        "   is an exit hatch. The drawback of tf.py_function is that it's not portable or particularly\n",
        "   performant, cannot be saved with SavedModel, and does not work well in distributed\n",
        "   (multi-GPU, TPU) setups. Also, since tf.py_function has to be wired into the graph,\n",
        "   it casts all inputs/outputs to tensors.\n",
        "\"\"\"\n",
        "   #Input needs to be reshaped properly at the end (common problem in Computer Vision)"
      ],
      "metadata": {
        "id": "sLKkreMgKpuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Changing Python global and free variables\n",
        "# Changing Python global and free variables counts as a Python side effect, so it\n",
        "# only happens during tracing.\n",
        "\n",
        "external_list = []\n",
        "\n",
        "@tf.function\n",
        "def side_effect(x):\n",
        "  print('Python side effect')\n",
        "  external_list.append(x)\n",
        "\n",
        "\n",
        "side_effect(1)\n",
        "side_effect(1)\n",
        "side_effect(1)\n",
        "# The list append only happened once!\n",
        "print(len(external_list)) # items are added only once\n",
        "\n",
        "# In summary, as a rule of thumb, you should avoid mutating python objects such as\n",
        "# integers or containers like lists that live outside the Function. Instead, use\n",
        "# arguments and TF objects."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQEp7WgnMIUZ",
        "outputId": "8b763f81-dbcd-4631-b09f-1a05bc3e2233"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python side effect\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem with Python Iterators and Generators\n",
        "# Many Python features, such as generators and iterators, rely on the Python runtime\n",
        "# to keep track of state. In general, while these constructs work as expected in\n",
        "# eager mode, they are examples of Python side effects and therefore only happen\n",
        "# during tracing.\n",
        "\n",
        "# Better use the tf.data API can help implement generator patterns\n",
        "# => tf.data.Dataset.from_tensor_slices"
      ],
      "metadata": {
        "id": "SqwIsKBlOF6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All outputs of a tf.function must be return values\n",
        "\"\"\"\n",
        "With the exception of tf.Variables, a tf.function must return all its outputs.\n",
        "Attempting to directly access any tensors from a function without going through\n",
        "return values causes \"leaks\".\n",
        "\"\"\"\n",
        "# This is true even if the leaked value is also returned:\n",
        "\n",
        "x = None\n",
        "\n",
        "@tf.function\n",
        "def leaky_function(a):\n",
        "  global x\n",
        "  x = a + 1  # Bad - leaks local tensor\n",
        "  return a + 2\n",
        "\n",
        "correct_a = leaky_function(tf.constant(1))\n",
        "\n",
        "print(correct_a.numpy())  # Good - value obtained from function's returns\n",
        "try:\n",
        "  x.numpy()  # Bad - tensor leaked from inside the function, cannot be used here\n",
        "except AttributeError as expected:\n",
        "  print(expected)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-QOMNwNOmAI",
        "outputId": "e501094f-df2e-4fe7-aecc-2a80b7cd745c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "'Tensor' object has no attribute 'numpy'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Usually, leaks such as these occur when you use Python statements or data structures.\n",
        "In addition to leaking inaccessible tensors, such statements are also likely wrong\n",
        "because they count as Python side effects, and are not guaranteed to execute at\n",
        "every function call.\n",
        "\n",
        "Common ways to leak local tensors also include mutating an external Python collection, or an object:\n",
        "\"\"\"\n",
        "class MyClass:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.field = None\n",
        "\n",
        "external_list = []\n",
        "external_object = MyClass()\n",
        "\n",
        "def leaky_function():\n",
        "  a = tf.constant(1)\n",
        "  external_list.append(a)  # Bad - leaks tensor\n",
        "  external_object.field = a  # Bad - leaks tensor"
      ],
      "metadata": {
        "id": "8D4IFzEWPVP5"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recursive tf.functions are not supported\n",
        "# Recursive Functions are not supported and could cause infinite loops."
      ],
      "metadata": {
        "id": "P3QwvLyFPh8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Known Issues in tf.function\n",
        "# refer here: https://www.tensorflow.org/guide/function#known_issues"
      ],
      "metadata": {
        "id": "UvkIBpr4Pqrf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}